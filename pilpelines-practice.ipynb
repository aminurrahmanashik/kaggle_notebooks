{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1da15b1",
   "metadata": {
    "papermill": {
     "duration": 0.015378,
     "end_time": "2021-08-18T18:40:58.114179",
     "exception": false,
     "start_time": "2021-08-18T18:40:58.098801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this particular notebook, we're gonna learn about pipelines which can be used to keep our data preprocessing and modeling code organized. (bundles the preprocessing and modeling steps)<br>\n",
    "This notebook has been created by inspiring from this [notebook](https://www.kaggle.com/alexisbcook/pipelines).<br>\n",
    "We're gonna use the dataset from [Melbourne Housing Snapshot](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot/home)<br>\n",
    "We won't focus on the data loading step. Instead, you can imagine you are at a point where you already have the training and validation data in X_train, X_valid, y_train, and y_valid.<br>\n",
    "Yes, let's have a start...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e156ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:58.147559Z",
     "iopub.status.busy": "2021-08-18T18:40:58.146588Z",
     "iopub.status.idle": "2021-08-18T18:40:59.205247Z",
     "shell.execute_reply": "2021-08-18T18:40:59.204574Z",
     "shell.execute_reply.started": "2021-08-18T18:28:25.641848Z"
    },
    "papermill": {
     "duration": 1.075922,
     "end_time": "2021-08-18T18:40:59.205386",
     "exception": false,
     "start_time": "2021-08-18T18:40:58.129464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a67674",
   "metadata": {
    "papermill": {
     "duration": 0.015123,
     "end_time": "2021-08-18T18:40:59.235520",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.220397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "See how this dataset looks.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110d35f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:59.277889Z",
     "iopub.status.busy": "2021-08-18T18:40:59.277198Z",
     "iopub.status.idle": "2021-08-18T18:40:59.301973Z",
     "shell.execute_reply": "2021-08-18T18:40:59.301451Z",
     "shell.execute_reply.started": "2021-08-18T18:28:25.811238Z"
    },
    "papermill": {
     "duration": 0.048943,
     "end_time": "2021-08-18T18:40:59.302087",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.253144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12167</th>\n",
       "      <td>u</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3182.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>-37.85984</td>\n",
       "      <td>144.9867</td>\n",
       "      <td>13240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>h</td>\n",
       "      <td>SA</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.85800</td>\n",
       "      <td>144.9005</td>\n",
       "      <td>6380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.79880</td>\n",
       "      <td>144.8220</td>\n",
       "      <td>3755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>u</td>\n",
       "      <td>SP</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3046.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>-37.70830</td>\n",
       "      <td>144.9158</td>\n",
       "      <td>8870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Western Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>-37.76230</td>\n",
       "      <td>144.8272</td>\n",
       "      <td>4217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n",
       "6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n",
       "8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n",
       "2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n",
       "6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n",
       "\n",
       "       Bathroom  Car  Landsize  BuildingArea  YearBuilt  Lattitude  \\\n",
       "12167       1.0  1.0       0.0           NaN     1940.0  -37.85984   \n",
       "6524        2.0  1.0     193.0           NaN        NaN  -37.85800   \n",
       "8413        1.0  1.0     555.0           NaN        NaN  -37.79880   \n",
       "2919        1.0  1.0     265.0           NaN     1995.0  -37.70830   \n",
       "6043        1.0  2.0     673.0         673.0     1970.0  -37.76230   \n",
       "\n",
       "       Longtitude  Propertycount  \n",
       "12167    144.9867        13240.0  \n",
       "6524     144.9005         6380.0  \n",
       "8413     144.8220         3755.0  \n",
       "2919     144.9158         8870.0  \n",
       "6043     144.8272         4217.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1f5ef",
   "metadata": {
    "papermill": {
     "duration": 0.015165,
     "end_time": "2021-08-18T18:40:59.332432",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.317267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Unlike the Missing values and categorical variables we did earlier with, we're gonna show the whole process of pipelining in 3 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3c966",
   "metadata": {
    "papermill": {
     "duration": 0.015831,
     "end_time": "2021-08-18T18:40:59.363980",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.348149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1: Define Preprocessing Steps.\n",
    "Actally, to bundle together the the different preprocessing steps and modeling steps, we're gonna use the **ColumnTransformer**.<br>\n",
    "The code we'll write below which- \n",
    "* Imputes missing values in *numerical* data, and\n",
    "* Imputes missing values and applies a one-hot-encoding to *categorical* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df33136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:59.398832Z",
     "iopub.status.busy": "2021-08-18T18:40:59.398017Z",
     "iopub.status.idle": "2021-08-18T18:40:59.533656Z",
     "shell.execute_reply": "2021-08-18T18:40:59.532989Z",
     "shell.execute_reply.started": "2021-08-18T18:28:25.847279Z"
    },
    "papermill": {
     "duration": 0.154484,
     "end_time": "2021-08-18T18:40:59.533909",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.379425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first import all the necessary things \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451212b",
   "metadata": {
    "papermill": {
     "duration": 0.015916,
     "end_time": "2021-08-18T18:40:59.566468",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.550552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Necessary libraries have been imported.<br>\n",
    "Now, we're gonna call for an imputer for numerical data.<br>\n",
    "We also keep the strategy as *constant*, fill_value is used to replace all occurrences of missing_values. [see here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d1292d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:59.603003Z",
     "iopub.status.busy": "2021-08-18T18:40:59.602124Z",
     "iopub.status.idle": "2021-08-18T18:40:59.605000Z",
     "shell.execute_reply": "2021-08-18T18:40:59.604500Z",
     "shell.execute_reply.started": "2021-08-18T18:28:26.008152Z"
    },
    "papermill": {
     "duration": 0.022936,
     "end_time": "2021-08-18T18:40:59.605122",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.582186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy = 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39cfd6",
   "metadata": {
    "papermill": {
     "duration": 0.015826,
     "end_time": "2021-08-18T18:40:59.637007",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.621181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "preprocessing for categorical data(defined what should we do in above cell at step1)<br>\n",
    "As stated above, we're gonna use both the imputer and one-hot-encoding in pipelining.<br>\n",
    "We named the imputation as '*categ_imputer*' and one-hot-encoding as '*categ_onehot*' and did inside a list as tuple which is under the **Pipeline**.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76bc8e1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:59.671245Z",
     "iopub.status.busy": "2021-08-18T18:40:59.670713Z",
     "iopub.status.idle": "2021-08-18T18:40:59.674304Z",
     "shell.execute_reply": "2021-08-18T18:40:59.674794Z",
     "shell.execute_reply.started": "2021-08-18T18:28:26.015331Z"
    },
    "papermill": {
     "duration": 0.022085,
     "end_time": "2021-08-18T18:40:59.674935",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.652850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    ('categ_imputer', SimpleImputer(strategy = 'most_frequent')), \n",
    "    ('categ_onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923ac2f",
   "metadata": {
    "papermill": {
     "duration": 0.016617,
     "end_time": "2021-08-18T18:40:59.707266",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.690649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we said earlier, let's do bundle precessing numerical and categorical data.<br>\n",
    "We're naming our transformers as *num_trans* and *cate_trans*. num_trans is based on **numerical_cols** and cate_trans is based on **categorical_cols**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf48d28c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:59.741746Z",
     "iopub.status.busy": "2021-08-18T18:40:59.741160Z",
     "iopub.status.idle": "2021-08-18T18:40:59.744462Z",
     "shell.execute_reply": "2021-08-18T18:40:59.744921Z",
     "shell.execute_reply.started": "2021-08-18T18:28:26.030163Z"
    },
    "papermill": {
     "duration": 0.022052,
     "end_time": "2021-08-18T18:40:59.745063",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.723011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bundle_preprocessor = ColumnTransformer(\n",
    "transformers = [\n",
    "    ('num_trans', numerical_transformer, numerical_cols),\n",
    "    ('cate_trans', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bea288a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:59.781467Z",
     "iopub.status.busy": "2021-08-18T18:40:59.780888Z",
     "iopub.status.idle": "2021-08-18T18:40:59.799592Z",
     "shell.execute_reply": "2021-08-18T18:40:59.798947Z",
     "shell.execute_reply.started": "2021-08-18T18:28:26.041483Z"
    },
    "papermill": {
     "duration": 0.038904,
     "end_time": "2021-08-18T18:40:59.799722",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.760818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('num_trans',\n",
       "                                 SimpleImputer(strategy='constant'),\n",
       "                                 ['Rooms', 'Distance', 'Postcode', 'Bedroom2',\n",
       "                                  'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
       "                                  'YearBuilt', 'Lattitude', 'Longtitude',\n",
       "                                  'Propertycount']),\n",
       "                                ('cate_trans',\n",
       "                                 Pipeline(steps=[('categ_imputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('categ_onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['Type', 'Method', 'Regionname'])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bundle_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8464f3c",
   "metadata": {
    "papermill": {
     "duration": 0.016891,
     "end_time": "2021-08-18T18:40:59.835799",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.818908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2: Defining Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49ec21",
   "metadata": {
    "papermill": {
     "duration": 0.015919,
     "end_time": "2021-08-18T18:40:59.868383",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.852464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Just defining a model e.g randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e13a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:40:59.905517Z",
     "iopub.status.busy": "2021-08-18T18:40:59.904683Z",
     "iopub.status.idle": "2021-08-18T18:40:59.953183Z",
     "shell.execute_reply": "2021-08-18T18:40:59.952593Z",
     "shell.execute_reply.started": "2021-08-18T18:28:26.073154Z"
    },
    "papermill": {
     "duration": 0.068799,
     "end_time": "2021-08-18T18:40:59.953306",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.884507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators = 100, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf587da5",
   "metadata": {
    "papermill": {
     "duration": 0.01572,
     "end_time": "2021-08-18T18:40:59.985005",
     "exception": false,
     "start_time": "2021-08-18T18:40:59.969285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 3: Creating and Evaluating the Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b26f3",
   "metadata": {
    "papermill": {
     "duration": 0.015069,
     "end_time": "2021-08-18T18:41:00.016160",
     "exception": false,
     "start_time": "2021-08-18T18:41:00.001091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps.<br>\n",
    "Though, in previous we have seen the preprocessing for the dataset first and then defining model and then fitting dataset in the model. Then finally we do prediction.<br>\n",
    "*Eventually, MEA for each step. But now, we'll determine MEA only one time with everything of Preprocessing and model defining.*<br>\n",
    "There are a few important things to notice:\n",
    "\n",
    "* With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do *imputation*, *one-hot encoding*, and *model training* in separate steps. This becomes especially *messy* if we have to deal with both numerical and categorical variables!)\n",
    "* With the pipeline, we supply the unprocessed features in **X_valid** to the **predict()** command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.) <br>\n",
    "Now, we're defining the pipeline to bundle the preprocessing and modeling steps.<br>\n",
    "Inside the Pipeline class, we're passing preprocessor and model in a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0680f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:41:00.058399Z",
     "iopub.status.busy": "2021-08-18T18:41:00.054477Z",
     "iopub.status.idle": "2021-08-18T18:41:07.491604Z",
     "shell.execute_reply": "2021-08-18T18:41:07.492127Z",
     "shell.execute_reply.started": "2021-08-18T18:29:14.793436Z"
    },
    "papermill": {
     "duration": 7.460726,
     "end_time": "2021-08-18T18:41:07.492301",
     "exception": false,
     "start_time": "2021-08-18T18:41:00.031575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 160679.18917034855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', bundle_preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866379bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:41:07.533591Z",
     "iopub.status.busy": "2021-08-18T18:41:07.532658Z",
     "iopub.status.idle": "2021-08-18T18:41:07.534909Z",
     "shell.execute_reply": "2021-08-18T18:41:07.535351Z",
     "shell.execute_reply.started": "2021-08-18T18:29:31.358878Z"
    },
    "papermill": {
     "duration": 0.025088,
     "end_time": "2021-08-18T18:41:07.535507",
     "exception": false,
     "start_time": "2021-08-18T18:41:07.510419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# named preprocessor as preprocessor 'my_preprocessor' and model as 'my_model'\n",
    "my_pipelines = Pipeline(steps = [\n",
    "    ('my_preprocessor', bundle_preprocessor),\n",
    "    ('my_model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c729d",
   "metadata": {
    "papermill": {
     "duration": 0.016893,
     "end_time": "2021-08-18T18:41:07.570547",
     "exception": false,
     "start_time": "2021-08-18T18:41:07.553654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, we've defined our pipeline for the predicions.<br>\n",
    "Now, let's fit it and predict on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdef0cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:41:07.606999Z",
     "iopub.status.busy": "2021-08-18T18:41:07.606100Z",
     "iopub.status.idle": "2021-08-18T18:41:14.971295Z",
     "shell.execute_reply": "2021-08-18T18:41:14.971782Z",
     "shell.execute_reply.started": "2021-08-18T18:33:44.241937Z"
    },
    "papermill": {
     "duration": 7.38493,
     "end_time": "2021-08-18T18:41:14.971966",
     "exception": false,
     "start_time": "2021-08-18T18:41:07.587036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# as we've done everything in pipeline, we will use the main dataset (X_train/y_train) which were different in previous tutorials.\n",
    "my_pipelines.fit(X_train, y_train)\n",
    "\n",
    "preds = my_pipelines.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f65f9",
   "metadata": {
    "papermill": {
     "duration": 0.016244,
     "end_time": "2021-08-18T18:41:15.005157",
     "exception": false,
     "start_time": "2021-08-18T18:41:14.988913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MEA after doing pipelining..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a9d39e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-18T18:41:15.043001Z",
     "iopub.status.busy": "2021-08-18T18:41:15.042290Z",
     "iopub.status.idle": "2021-08-18T18:41:15.044857Z",
     "shell.execute_reply": "2021-08-18T18:41:15.045339Z",
     "shell.execute_reply.started": "2021-08-18T18:36:05.167399Z"
    },
    "papermill": {
     "duration": 0.023813,
     "end_time": "2021-08-18T18:41:15.045473",
     "exception": false,
     "start_time": "2021-08-18T18:41:15.021660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEA by pipelining: 160679.18917034855\n"
     ]
    }
   ],
   "source": [
    "print(\"MEA by pipelining:\", mean_absolute_error(preds, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6fbef",
   "metadata": {
    "papermill": {
     "duration": 0.016514,
     "end_time": "2021-08-18T18:41:15.081242",
     "exception": false,
     "start_time": "2021-08-18T18:41:15.064728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Congratulations!\n",
    "We're done with pipelining!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24.51076,
   "end_time": "2021-08-18T18:41:15.705664",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-18T18:40:51.194904",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
